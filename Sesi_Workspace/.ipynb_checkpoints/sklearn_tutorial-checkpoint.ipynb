{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9173d0-50be-4829-9f82-cae7babdf551",
   "metadata": {},
   "source": [
    "## Explore:\n",
    "1. sklearn.preprocessing?\n",
    "2. Pipeline sklearn?\n",
    "3. pipeline di dask dan pyspark?\n",
    "4. Referensi: Data pipeline and data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bae6a4-bd24-4f6b-b258-f28bad498480",
   "metadata": {},
   "source": [
    "## 1. Sklearn.preprocessing?\n",
    "sources: \n",
    "- https://scikit-learn.org/stable/getting_started.html\n",
    "- https://scikit-learn.org/stable/user_guide.html\n",
    "\n",
    "### Fitting and predicting: estimator basics\n",
    "\n",
    "- Estimator is built-in machine learning algorithms and models in ```Scikit-learn```. Each estimator can be fitted to some data using its *fit* method. So Estimator is a class.\n",
    "- The *fit* method generally accepts 2 inputs:\n",
    "    - The sample matrix (or design matrix) X. The size of X is typically (n_samples, n_features), which means that samples are represented as row and features are represented as coulumns.\n",
    "    - The target values y which are realt numbers for regression tasks, or integers for classification (or any other discrete set of values). Foor unsuperviszed learning tasks, y does not need to be specified. y is usually 1d array where the i th entry corresponds to the tartget of the i th sample (row) of X.\n",
    "- Both X and y are usually expected to be numpy arrays or equivalent array-like data types, though some estimators work with other formats such as sparse matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c8b282-e6a7-465a-86b4-b753d7a52c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example: fitting with Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0) # Dibahas di sesi Supervised Learning\n",
    "X = [[1,2,3], [11,12,13]] # 2 samples, 3 features\n",
    "y = [0,1] # classes of each sample\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94e13666-df7e-4ae3-9606-f3d8bdafb894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0 1]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "### Predicting target values of new data\n",
    "\n",
    "print(clf.predict(X))\n",
    "print(clf.predict([[4,5,6], [14,15,16]]))\n",
    "print(clf.predict([[14,5,16], [14,1,16]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a960d4-6cc3-4ab8-9900-f967d2b73d28",
   "metadata": {},
   "source": [
    "## 2. Pipelines: chaining pre-processors and estimators\n",
    "\n",
    "Transformers and estimators (predictors) can be combined together into a single unifying object: a **Pipeline**. The pipeline offers the same API as a regular estimator: it can be fitted and used for prediction with ```fit``` and ```predict```. As we will see later, using a pipeline will also prevent you from **data leakage**, i.e. disclosing some testing data in your training data.\n",
    "\n",
    "In the following example, we load the Iris dataset, split it into train and test sets, and compute the accuracy score of a pipeline on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd4ae59a-9e16-4930-879a-2a491f18d0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pipelines example\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "### Create a pipeline object\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "\n",
    "### Load the iris dataset and split it into train and test sets\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "\n",
    "### Fit the whole pipeline\n",
    "pipe.fit(X_train, y_train)\n",
    "#Pipeline(steps=[('standardscaler', StandardScaler()), ('logisticregression', LogisticRegression())])\n",
    "\n",
    "### We can now use it like any other estimator\n",
    "accuracy_score(pipe.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e247f-fa7e-437e-9a2a-ddf7f6e825cb",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Fitting a model to some data does not entail that it will predict well on unseen data. This needs to be directly evaluated. We have just seen the **train_test_split** helper that splits a dataset into train and test sets, but ```scikit-learn``` provides many other tools for model evaluation, in particular for **cross-validation**.\n",
    "\n",
    "Here how to perform a 5-fold cross-validation procedure is briefly shown, using the **cross_validate** helper. Note that it is also possible to manually iterate over the folds, use different data splitting strategies, and use custom scoring function. For further detail on this direction, please see User Guide: https://scikit-learn.org/stable/user_guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b40049e-0f5f-4df2-9cf7-f4621af72d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "X, y = make_regression(n_samples=1000, random_state=0)\n",
    "lr = LinearRegression()\n",
    "\n",
    "result = cross_validate(lr, X, y) # defaults to 5-folds Cross Validate\n",
    "result['test_score'] # r_squared score is high because dataset is easy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "828d7925-d1bb-490e-8fee-c2b0a4f34156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.03181028, 0.00324631, 0.0034883 , 0.00371099, 0.00341368]),\n",
       " 'score_time': array([0.00043678, 0.00040627, 0.00041699, 0.00050569, 0.00046539]),\n",
       " 'test_score': array([1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eac99f-1479-4756-af66-b1b0d35020ea",
   "metadata": {},
   "source": [
    "## Automatic paramter search\n",
    "All estimators have parameters (often called hyper-parameters in the literature) that can be tuned. The generalization power of an estimator often critically depends on a few parameters. For example a **RandomForestRegressor** has a *n_estimator* parameter that determines the number of trees in the forest, and a *max_dept* parameter that determines the maximum depth of each tree. Quite often, it is not clear what the exact values of these paramteres should be, since they depend on the data at hand.\n",
    "\n",
    "```Scikit-learn``` provides tools to automatically find the best parameter combinations (via cross-validation). In the following example, we randomly search over the parameter space of a random forest with a **RandomizedSearchCV** object. When the search is over, the RandomizedSearchCV behaves as a **RandomForestRegressor** that has been fitted with the best set of paramters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b55acb03-3008-4d16-8bcd-43e51a281a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0), n_iter=5,\n",
       "                   param_distributions={'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f6582c29d60>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f659b195940>},\n",
       "                   random_state=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint\n",
    "\n",
    "#X,y = fetch_california_housing(return_X_y=True)\n",
    "X,y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "\n",
    "# Define the parameter space that will be searched over\n",
    "param_distributions ={'n_estimators': randint(1,5), \n",
    "                     'max_depth': randint(5,10)}\n",
    "\n",
    "# Now create a searchCV object and fit it to the data\n",
    "search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0), \n",
    "                           n_iter=5,\n",
    "                           param_distributions=param_distributions,\n",
    "                           random_state=0)\n",
    "\n",
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "208217ef-7f2c-409a-8fb7-b42c75e01d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0), n_iter=5,\n",
       "                   param_distributions={'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f6582c29d60>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f659b195940>},\n",
       "                   random_state=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X)\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99464e21-0b94-452a-b25c-b9130b241cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9, 'n_estimators': 4}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01e3d1aa-9e6e-4c30-99db-ec8571a97101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9710365853658537"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The search object now acts like a normal random forest estimator, with max_depth=9 and n_estimators=4\n",
    "#print(X_test,y_test)\n",
    "search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e82c39b2-6f6c-44bd-b414-b6baa224e702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n",
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The search object now acts like a normal random forest estimator, with max_depth=9 and n_estimators=4\n",
    "print(pipe.predict(X_test))\n",
    "print(y_test)\n",
    "accuracy_score(pipe.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0a3b3-4164-42c3-a064-a35a24f34d79",
   "metadata": {},
   "source": [
    "## Data leakage\n",
    "References:\n",
    "- https://www.kaggle.com/alexisbcook/data-leakage    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fade2df-db0c-4b50-999b-a873e0b3fc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
